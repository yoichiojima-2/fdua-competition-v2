import textwrap

from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_fixed

from fdua_competition.baes_models import AnswerQueryOutput
from fdua_competition.logging_config import logger
from fdua_competition.models import create_chat_model
from fdua_competition.utils import dict_to_yaml
from fdua_competition.utils import before_sleep_hook


class CleanseResponseOutput(BaseModel):
    query: str = Field(..., title="The query string that was used to generate the answer.")
    input: str = Field(..., title="The raw answer output provided in the 'response' field.")
    output: str = Field(..., title="The cleansed 'response' string that satisfies the requirements.")


@retry(stop=stop_after_attempt(24), wait=wait_fixed(1), before_sleep=before_sleep_hook)
def cleanse_response(answer: AnswerQueryOutput) -> CleanseResponseOutput:
    role = textwrap.dedent(
        """
        You are an intelligent assistant specializing in text refinement.
        Your task is to cleanse and standardize the answer output generated by the answer_query function.
        
        ## Instructions:
        - Ensure the final "response" is within 54 tokens.
        - If the answer involves a count (何件), include only the numerical value.
        - If the answer is a choice between options (どちらか), state the answer in a simple and assertive manner.
        - Avoid polite or overly formal expressions (e.g., avoid endings like "です" or phrases like "の方が").
        - Use a direct, non-polite, and declarative style (for example, "XのYが多い" rather than "XのYの方が多いです").
        - The output should be a concise statement that directly declares the answer.
        
        ## Input:
        - **answer**: The raw answer output provided in the "response" field.
        
        ## Output:
        Return a cleansed "response" string that satisfies the above requirements.
        
        Ensure that your final output is within 54 tokens.
        """
    )

    chat_model = create_chat_model().with_structured_output(CleanseResponseOutput)
    prompt_template = ChatPromptTemplate.from_messages([("system", role), ("user", "answer: {answer}")])
    chain = prompt_template | chat_model
    res = chain.invoke({"answer": dict_to_yaml(answer.model_dump())})

    logger.info(f"[cleanse_answer_query]\n{dict_to_yaml(res.model_dump())}\n")
    return res
